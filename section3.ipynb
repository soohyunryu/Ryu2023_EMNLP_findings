{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 3. Comparison between humans and Transformer language models in plausibility processing patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_surprisal(sentence, target_word, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculate the surprisal of a target word within a given sentence using a specified model and tokenizer.\n",
    "    \n",
    "    Parameters:\n",
    "        sentence (str): The sentence within which the word exists.\n",
    "        target_word (str): The specific word whose surprisal to be calculated.\n",
    "        model (torch.nn.Module): The PyTorch model used for prediction.\n",
    "        tokenizer: The tokenizer used for word tokenization.\n",
    "\n",
    "    Returns:\n",
    "        surprisal_value (float): The surprisal of the target word.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the sentence\n",
    "    indexed_tokens = tokenizer.encode(sentence)\n",
    "    tokens = [tokenizer.decode(index).strip().replace(' ','') for index in indexed_tokens]\n",
    "\n",
    "\n",
    "    # Also move the model to the same device\n",
    "    #model.to(device)\n",
    "    if(target_word in tokens):\n",
    "    \n",
    "        # Locate the target word within the tokens\n",
    "        target_word_location = tokens.index(target_word)\n",
    "\n",
    "        # Index the token prefixes up to the target word location\n",
    "        prefix_index = indexed_tokens[:target_word_location]\n",
    "\n",
    "        # Convert the prefix tokens to tensor and move the tensor to device\n",
    "        prefix_tensor = torch.tensor([prefix_index])\n",
    "\n",
    "    # Make predictions with the model while excluding gradient computation\n",
    "        with torch.no_grad():\n",
    "            output = model(prefix_tensor)\n",
    "            predictions = torch.nn.functional.softmax(output[0], dim=-1)\n",
    "            prediction_result = predictions[0, -1, :]\n",
    "        \n",
    "            # Calculate the surprisal of the target word\n",
    "            score_of_target_word = prediction_result[indexed_tokens[target_word_location]]\n",
    "            surprisal_of_target_word = -torch.log2(score_of_target_word).numpy()\n",
    "\n",
    "        return float(surprisal_of_target_word)\n",
    "    else:\n",
    "        return None;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
      "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "## Loading Models\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import AlbertTokenizer, AlbertForMaskedLM\n",
    "from transformers import BertTokenizer, BertLMHeadModel\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model_gpt2 = GPT2LMHeadModel.from_pretrained('gpt2', output_attentions = True)\n",
    "\n",
    "tokenizer_Albert = AlbertTokenizer.from_pretrained('albert-base-v2', output_attentions = True)\n",
    "model_Albert = AlbertForMaskedLM.from_pretrained('albert-base-v2')\n",
    "\n",
    "tokenizer_BERT = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_BERT = BertLMHeadModel.from_pretrained('bert-base-uncased',output_attentions = True)\n",
    "\n",
    "tokenizer_roBERTa = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model_roBERTa = RobertaForMaskedLM.from_pretrained(\"roberta-base\", output_attentions = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique IDs.\n",
    "cunnings2018 = pd.read_csv(\"cunnings2018.csv\")\n",
    "unique_ids = np.unique(cunnings2018[\"id\"])\n",
    "\n",
    "# Loop through each unique id. \n",
    "for id in unique_ids:\n",
    "    # Filter rows for the specific 'id'.\n",
    "    data_set = cunnings2018[cunnings2018['id'] == id]\n",
    "    \n",
    "    # Collect specific sentences based on 'cond' field.\n",
    "    sentence_plpl = data_set[data_set['cond'] == 'pl_pl'].sent.item()\n",
    "    sentence_plimpl = data_set[data_set['cond'] == 'pl_impl'].sent.item()\n",
    "    sentence_implpl = data_set[data_set['cond'] == 'impl_pl'].sent.item()\n",
    "    sentence_implimpl = data_set[data_set['cond'] == 'impl_impl'].sent.item()\n",
    "    \n",
    "    # Collect other specific fields based on 'cond' field.\n",
    "    pl_subj = data_set[data_set['cond'] == 'pl_impl'].subj.item()\n",
    "    impl_subj = data_set[data_set['cond'] == 'impl_impl'].subj.item()\n",
    "    pl_dist = data_set[data_set['cond'] == 'pl_pl'].dist.item()\n",
    "    impl_dist = data_set[data_set['cond'] == 'pl_impl'].dist.item()\n",
    "\n",
    "    # Get the 'verb' field\n",
    "    verb = data_set[data_set['cond'] == 'pl_impl'].verb.item()\n",
    "    #print(id, sentence_plpl, verb)\n",
    "    \n",
    "    # Tokenize sentences.\n",
    "    tokens_plpl = tokenizer_Albert.encode(sentence_plpl)\n",
    "    tokens_implimpl = tokenizer_Albert.encode(sentence_implimpl)\n",
    "    plpl_tokens = [tokenizer_Albert.decode(token).strip().replace(' ','')  for token in tokens_plpl]\n",
    "    implimpl_tokens = [tokenizer_Albert.decode(token).strip().replace(' ','')  for token in tokens_implimpl]\n",
    "    \n",
    "    plpl_surprisal_albert = calculate_surprisal(sentence_plpl, verb, model_Albert, tokenizer_Albert)\n",
    "    plimpl_surprisal_albert = calculate_surprisal(sentence_plimpl, verb, model_Albert,tokenizer_Albert)\n",
    "    implpl_surprisal_albert = calculate_surprisal(sentence_implpl, verb, model_Albert, tokenizer_Albert)\n",
    "    implimpl_surprisal_albert = calculate_surprisal(sentence_implimpl, verb, model_Albert, tokenizer_Albert)\n",
    "    \n",
    "    plpl_surprisal_bert = calculate_surprisal(sentence_plpl, verb, model_BERT, tokenizer_BERT)\n",
    "    plimpl_surprisal_bert = calculate_surprisal(sentence_plimpl, verb, model_BERT,tokenizer_BERT)\n",
    "    implpl_surprisal_bert = calculate_surprisal(sentence_implpl, verb, model_BERT, tokenizer_BERT)\n",
    "    implimpl_surprisal_bert = calculate_surprisal(sentence_implimpl, verb, model_BERT, tokenizer_BERT)\n",
    "    \n",
    "    plpl_surprisal_gpt = calculate_surprisal(sentence_plpl, verb, model_gpt2, tokenizer_gpt2)\n",
    "    plimpl_surprisal_gpt = calculate_surprisal(sentence_plimpl, verb, model_gpt2,tokenizer_gpt2)\n",
    "    implpl_surprisal_gpt = calculate_surprisal(sentence_implpl, verb, model_gpt2, tokenizer_gpt2)\n",
    "    implimpl_surprisal_gpt = calculate_surprisal(sentence_implimpl, verb, model_gpt2, tokenizer_gpt2)\n",
    "\n",
    "    plpl_surprisal_roBERTa = calculate_surprisal(sentence_plpl, verb, model_roBERTa, tokenizer_roBERTa)\n",
    "    plimpl_surprisal_roBERTa = calculate_surprisal(sentence_plimpl, verb, model_roBERTa,tokenizer_roBERTa)\n",
    "    implpl_surprisal_roBERTa = calculate_surprisal(sentence_implpl, verb, model_roBERTa, tokenizer_roBERTa)\n",
    "    implimpl_surprisal_roBERTa = calculate_surprisal(sentence_implimpl, verb, model_roBERTa, tokenizer_roBERTa)\n",
    "\n",
    "    \n",
    "    # Set the 'surprisal' score for each 'cond' in the original DataFrame 'cunnings2018'.\n",
    "    cunnings2018.loc[(cunnings2018['id'] == id) & (cunnings2018['cond'] == 'pl_pl'), 'surprisal_Albert'] = plpl_surprisal_albert\n",
    "    cunnings2018.loc[(cunnings2018['id'] == id) & (cunnings2018['cond'] == 'impl_pl'), 'surprisal_Albert'] = plimpl_surprisal_albert\n",
    "    cunnings2018.loc[(cunnings2018['id'] == id) & (cunnings2018['cond'] == 'pl_impl'), 'surprisal_Albert'] = implpl_surprisal_albert\n",
    "    cunnings2018.loc[(cunnings2018['id'] == id) & (cunnings2018['cond'] == 'impl_impl'), 'surprisal_Albert'] = implimpl_surprisal_albert\n",
    "    \n",
    "    # Set the 'surprisal' score for each 'cond' in the original DataFrame 'cunnings2018'.\n",
    "    cunnings2018.loc[(cunnings2018['id'] == id) & (cunnings2018['cond'] == 'pl_pl'), 'surprisal_BERT'] = plpl_surprisal_bert\n",
    "    cunnings2018.loc[(cunnings2018['id'] == id) & (cunnings2018['cond'] == 'impl_pl'), 'surprisal_BERT'] = implpl_surprisal_bert\n",
    "    cunnings2018.loc[(cunnings2018['id'] == id) & (cunnings2018['cond'] == 'pl_impl'), 'surprisal_BERT'] = plimpl_surprisal_bert\n",
    "    cunnings2018.loc[(cunnings2018['id'] == id) & (cunnings2018['cond'] == 'impl_impl'), 'surprisal_BERT'] = implimpl_surprisal_bert\n",
    "        \n",
    "    cunnings2018.loc[(cunnings2018['id'] == id) & (cunnings2018['cond'] == 'pl_pl'), 'surprisal_gpt2'] = plpl_surprisal_gpt\n",
    "    cunnings2018.loc[(cunnings2018['id'] == id) & (cunnings2018['cond'] == 'impl_pl'), 'surprisal_gpt2'] = implpl_surprisal_gpt\n",
    "    cunnings2018.loc[(cunnings2018['id'] == id) & (cunnings2018['cond'] == 'pl_impl'), 'surprisal_gpt2'] = plimpl_surprisal_gpt\n",
    "    cunnings2018.loc[(cunnings2018['id'] == id) & (cunnings2018['cond'] == 'impl_impl'), 'surprisal_gpt2'] = implimpl_surprisal_gpt\n",
    "\n",
    "\n",
    "    cunnings2018.loc[(cunnings2018['id'] == id) & (cunnings2018['cond'] == 'pl_pl'), 'surprisal_roBERTa'] = plpl_surprisal_roBERTa\n",
    "    cunnings2018.loc[(cunnings2018['id'] == id) & (cunnings2018['cond'] == 'impl_pl'), 'surprisal_roBERTa'] = implpl_surprisal_roBERTa\n",
    "    cunnings2018.loc[(cunnings2018['id'] == id) & (cunnings2018['cond'] == 'pl_impl'), 'surprisal_roBERTa'] = plimpl_surprisal_roBERTa\n",
    "    cunnings2018.loc[(cunnings2018['id'] == id) & (cunnings2018['cond'] == 'impl_impl'), 'surprisal_roBERTa'] = implimpl_surprisal_roBERTa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cunnings2018.to_csv('surprisals.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
